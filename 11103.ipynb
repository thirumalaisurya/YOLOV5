{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cc3799e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Admin/Documents/10.01.2022 spot/accident\\11103.ipynb\n",
      "C:/Users/Admin/Documents/10.01.2022 spot/accident\\output.mp4\n",
      "C:/Users/Admin/Documents/10.01.2022 spot/accident\\US_Accidents_Dec20_Updated.csv\n",
      "C:/Users/Admin/Documents/10.01.2022 spot/accident\\US_Accidents_Dec20_Updated.csv.zip\n",
      "C:/Users/Admin/Documents/10.01.2022 spot/accident\\US_Accidents_Dec20_Updated.rar\n",
      "C:/Users/Admin/Documents/10.01.2022 spot/accident\\.ipynb_checkpoints\\11103-checkpoint.ipynb\n",
      "C:/Users/Admin/Documents/10.01.2022 spot/accident\\.ipynb_checkpoints\\Untitled-checkpoint.ipynb\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('C:/Users/Admin/Documents/10.01.2022 spot/accident'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d20a6e0f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_7064/2850066838.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mgc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcategory_encoders\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mce\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\category_encoders\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcategory_encoders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjames_stein\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mJamesSteinEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcategory_encoders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat_boost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCatBoostEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcategory_encoders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglmm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mGLMMEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[0m__version__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'2.3.0'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\category_encoders\\glmm.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcategory_encoders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mordinal\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mOrdinalEncoder\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcategory_encoders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformula\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msmf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbayes_mixed_glm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBinomialBayesMixedGLM\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbgmm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\statsmodels\\formula\\api.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mlm_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscrete\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdiscrete_model\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdm_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregression\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmixed_linear_model\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mmlm_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenmod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgeneralized_linear_model\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mglm_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrobust\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrobust_linear_model\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mroblm_\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\statsmodels\\regression\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0myule_walker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_testing\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPytestTester\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0m__all__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'yule_walker'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'test'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\site-packages\\statsmodels\\regression\\linear_model.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     44\u001b[0m from statsmodels.tools.decorators import (cache_readonly,\n\u001b[0;32m     45\u001b[0m                                           cache_writable)\n\u001b[1;32m---> 46\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrapper\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwrap\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0memplike\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0melregress\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_ELRegOpts\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\importlib\\_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[1;34m(self, fullname)\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\lib\\importlib\\_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(self, path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Warning Libraries :\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Scientific and Data Manipulation Libraries :\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import gc\n",
    "import os\n",
    "import category_encoders as ce\n",
    "\n",
    "\n",
    "# ML Libraries :\n",
    "from sklearn.preprocessing            import LabelEncoder, OneHotEncoder \n",
    "from sklearn.preprocessing            import StandardScaler, MinMaxScaler, Normalizer, RobustScaler, MaxAbsScaler\n",
    "from sklearn.model_selection          import KFold, StratifiedKFold, train_test_split, cross_val_score\n",
    "from sklearn.linear_model             import LogisticRegression\n",
    "from sklearn                          import tree\n",
    "from sklearn.ensemble                 import RandomForestClassifier\n",
    "from sklearn.metrics                  import accuracy_score\n",
    "from sklearn.metrics                  import f1_score,precision_score\n",
    "#from sklearn.metrics                 import jaccard_similarity_score, jaccard_score  \n",
    "\n",
    "# Boosting Algorithms :\n",
    "from xgboost                          import XGBClassifier\n",
    "from lightgbm                         import LGBMClassifier\n",
    "\n",
    "# Data Visualization Libraries :\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.io as pio\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdee964",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3ef7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the data\n",
    "state_lst=['FL']\n",
    "df = pd.read_csv('C:/Users/Admin/Documents/10.01.2022 spot/accident/US_Accidents_Dec20_Updated.csv')\n",
    "df = df[df.State.isin(state_lst)]\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f31203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Descriptive Statistics of data :\n",
    "\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769e022",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerics = ['int16', 'int32','int64', 'float16','float32','float64']\n",
    "numeric_df = df.select_dtypes(include=numerics)\n",
    "len(numeric_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd86a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dea1031",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_value = df.isnull().sum().sort_values(ascending=False) /len(df)\n",
    "missing_percentages = missing_value * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ba3ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Percentages of missing values per columns: \\n\",missing_percentages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9dc888",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_percentages[missing_percentages != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58de4a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "missing_percentages[missing_percentages != 0].plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005a6a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('Number', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d841f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10a4c9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf36a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "df = pd.read_csv('C:/Users/Admin/Documents/10.01.2022 spot/accident/US_Accidents_Dec20_Updated.csv')\n",
    "# looking at the dataset\n",
    "print(df.head())\n",
    "print(df.info())\n",
    "#print(housing.ocean_proximity.value_counts())\n",
    "print(df.describe())\n",
    "\n",
    "# spliting the data into training and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# visualizing the histogram for income categories\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cb187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stratified sampling\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\n",
    "for train_index, test_index in split.split(df, df[\"Railway\"]):\n",
    "    strat_train_set = df.loc[train_index]\n",
    "    strat_test_set = df.loc[test_index]\n",
    "    \n",
    "for set_ in (strat_train_set, strat_test_set):\n",
    "    set_.drop('Railway', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e4f36c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = strat_train_set.copy()\n",
    "#print(accident.head())\n",
    "\n",
    "# visualizing geographical data\n",
    "df.plot(kind='scatter', x='Start_Lat', y='End_Lng', alpha=0.4, s=df['Temperature(F)']/100, label='Temperature(F)',\n",
    "figsize=(12, 8), c='Humidity(%)', cmap=plt.get_cmap('jet'), colorbar=True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33513b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.City"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f22bc5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = df.City.unique()\n",
    "len(cities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07502433",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_by_accident = df.City.value_counts()\n",
    "cities_by_accident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e90c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 30 cities by number of accident\n",
    "cities_by_accident[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36094f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_by_accident[:25].plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11ca1d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2146c49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(cities_by_accident[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa18269",
   "metadata": {},
   "outputs": [],
   "source": [
    "high_accident_cities = cities_by_accident[cities_by_accident >= 1000]\n",
    "low_accident_cities = cities_by_accident[cities_by_accident < 1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fde98a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(high_accident_cities) /len(cities)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe46467",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(high_accident_cities, log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630b5b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(low_accident_cities) /len(cities)) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd2a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(low_accident_cities, log_scale=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b36db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_by_accident[cities_by_accident == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11f929f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.Start_Time[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08662229",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Start_Time = pd.to_datetime(df.Start_Time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb052628",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Start_Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517cae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to get hour from the timestamp\n",
    "df.Start_Time.dt.hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0cd6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df.Start_Time.dt.hour, bins=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5be919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df.Start_Time.dt.dayofweek, bins=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdfeec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunday_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 6]\n",
    "sns.histplot(sunday_start_time.dt.hour, bins=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc06fd39",
   "metadata": {},
   "outputs": [],
   "source": [
    "monday_start_time = df.Start_Time[df.Start_Time.dt.dayofweek == 0]\n",
    "sns.histplot(monday_start_time.dt.hour, bins=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a461fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(df.Start_Time.dt.month, bins=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f56588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data for yar 2019\n",
    "df_2019 = df[df.Start_Time.dt.year == 2019]\n",
    "sns.histplot(df_2019.Start_Time.dt.month, bins=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42e167aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time'] = pd.to_datetime(df.Start_Time, format='%Y-%m-%d %H:%M:%S')\n",
    "df = df.set_index('time')\n",
    "#set_index\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d0682b",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_text = {'D':'Daily','W':'Weekly','Y':'Yearly'}\n",
    "\n",
    "plt.subplots(1,3,figsize=(21,7))\n",
    "for i, (fr,text) in enumerate(freq_text.items(),1):\n",
    "    plt.subplot(1,3,i)\n",
    "    sample = df.ID['2016':].resample(fr).count()\n",
    "    sample.plot(style='.')\n",
    "    plt.title('Accidents, {} count'.format(text))\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Accident Count');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22002b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take smaller sample from the data\n",
    "sample_df = df.sample(int(0.1 * len(df)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e071c8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=sample_df.Start_Lng, y=sample_df.Start_Lat, size=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159e5c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from folium.plugins import HeatMapWithTime\n",
    "import folium\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, OrderedDict\n",
    "df = pd.read_csv('C:/Users/Admin/Documents/10.01.2022 spot/accident/US_Accidents_Dec20_Updated.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26e8322",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df[:100000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368462e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(mean_lat, mean_lng) = (data['End_Lat'].mean(), data['End_Lng'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a5d293",
   "metadata": {},
   "outputs": [],
   "source": [
    "dated_dict = defaultdict(list)\n",
    "for x,y in enumerate(data['Start_Time']):\n",
    "    dated_dict[y.split(' ')[0]].append([data.iloc[x,4],data.iloc[x,5]])\n",
    "\n",
    "dated_dict = OrderedDict(sorted(dated_dict.items(),key=lambda x:x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5052774f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickup_map = folium.Map((mean_lat, mean_lng), zoom_start=4, tiles=\"Stamen Terrain\")\n",
    "\n",
    "hourly_pickups = HeatMapWithTime(\n",
    "    data=list(dated_dict.values()),\n",
    "    index=list(dated_dict.keys()), \n",
    "    auto_play=True,\n",
    "    max_opacity=0.4,\n",
    "    radius=40\n",
    ")\n",
    "hourly_pickups.add_to(pickup_map)\n",
    "pickup_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c72b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_unique(data):\n",
    "    for column in data.columns :\n",
    "        \n",
    "        print(\"No of Unique Values in \"+column+\" Column are : \"+str(data[column].nunique()))\n",
    "        print(\"Actual Unique Values in \"+column+\" Column are : \"+str(data[column].sort_values(ascending=True,na_position='last').unique() ))\n",
    "        print(\"\")\n",
    "        \n",
    "display_unique(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "011a09f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast Start_Time to datetime \n",
    "\n",
    "df[\"Start_Time\"] = pd.to_datetime(df[\"Start_Time\"])\n",
    "\n",
    "# Extract year, month, weekday and day\n",
    "df[\"Year\"] = df[\"Start_Time\"].dt.year\n",
    "df[\"Month\"] = df[\"Start_Time\"].dt.month\n",
    "df[\"Weekday\"] = df[\"Start_Time\"].dt.weekday\n",
    "df[\"Day\"] = df[\"Start_Time\"].dt.day\n",
    "\n",
    "# Extract hour and minute\n",
    "df[\"Hour\"] = df[\"Start_Time\"].dt.hour\n",
    "df[\"Minute\"] = df[\"Start_Time\"].dt.minute\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8c8119",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.parallel_categories(df[[\"Side\", \"City\", \"Weekday\", \"Day\",\"Hour\",\"Minute\", \"Civil_Twilight\",\n",
    "                                   \"Severity\"]], \n",
    "                             color=\"Severity\", \n",
    "                             color_continuous_scale=px.colors.sequential.Aggrnyl  )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a22276e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,10))\n",
    "sns.heatmap(df.corr(), annot = True);\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaced2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_drop = [\"ID\", \"Start_Time\", \"End_Time\", \"End_Lat\", \"End_Lng\",\"Description\", \"Number\", \"Street\", \"County\", \"State\", \"Zipcode\",\n",
    "                    \"Country\", \"Timezone\", \"Airport_Code\", \"Weather_Timestamp\", \"Wind_Chill(F)\", \"Turning_Loop\", \"Sunrise_Sunset\", \"Nautical_Twilight\", \"Astronomical_Twilight\",\"City\",\"Civil_Twilight\",\"Bump\",\"Give_Way\",\"No_Exit\",\"Roundabout\",\"Traffic_Calming\"]\n",
    "df=df.drop(features_to_drop, axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb05a025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicate(data):\n",
    "    \n",
    "    print(\"BEFORE REMOVING DUPLICATES - No. of Rows = \",data.shape[0])\n",
    "    data.drop_duplicates(keep=\"first\", inplace=True) \n",
    "    print(\"AFTER REMOVING DUPLICATES  - No. of Rows = \",data.shape[0])\n",
    "    \n",
    "    return \"Checked Duplicates\"\n",
    "\n",
    "# Remove Duplicates from data :\n",
    "\n",
    "remove_duplicate(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4962df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_weather = df[\"Weather_Condition\"].unique()\n",
    "\n",
    "print(len(unique_weather))\n",
    "print(unique_weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad61643d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"Weather_Condition\"].str.contains(\"Thunder|T-Storm\", na=False), \"Weather_Condition\"] = \"Thunderstorm\"\n",
    "df.loc[df[\"Weather_Condition\"].str.contains(\"Snow|Sleet|Wintry\", na=False), \"Weather_Condition\"] = \"Snow\"\n",
    "df.loc[df[\"Weather_Condition\"].str.contains(\"Rain|Drizzle|Shower\", na=False), \"Weather_Condition\"] = \"Rain\"\n",
    "df.loc[df[\"Weather_Condition\"].str.contains(\"Wind|Squalls\", na=False), \"Weather_Condition\"] = \"Windy\"\n",
    "df.loc[df[\"Weather_Condition\"].str.contains(\"Hail|Pellets\", na=False), \"Weather_Condition\"] = \"Hail\"\n",
    "df.loc[df[\"Weather_Condition\"].str.contains(\"Fair\", na=False), \"Weather_Condition\"] = \"Clear\"\n",
    "df.loc[df[\"Weather_Condition\"].str.contains(\"Cloud|Overcast\", na=False), \"Weather_Condition\"] = \"Cloudy\"\n",
    "df.loc[df[\"Weather_Condition\"].str.contains(\"Mist|Haze|Fog\", na=False), \"Weather_Condition\"] = \"Fog\"\n",
    "df.loc[df[\"Weather_Condition\"].str.contains(\"Sand|Dust\", na=False), \"Weather_Condition\"] = \"Sand\"\n",
    "df.loc[df[\"Weather_Condition\"].str.contains(\"Smoke|Volcanic Ash\", na=False), \"Weather_Condition\"] = \"Smoke\"\n",
    "df.loc[df[\"Weather_Condition\"].str.contains(\"N/A Precipitation\", na=False), \"Weather_Condition\"] = np.nan\n",
    "\n",
    "print(df[\"Weather_Condition\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1564fce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Wind_Direction\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41ffd09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[df[\"Wind_Direction\"] == \"CALM\", \"Wind_Direction\"] = \"Calm\"\n",
    "df.loc[df[\"Wind_Direction\"] == \"VAR\", \"Wind_Direction\"] = \"Variable\"\n",
    "df.loc[df[\"Wind_Direction\"] == \"East\", \"Wind_Direction\"] = \"E\"\n",
    "df.loc[df[\"Wind_Direction\"] == \"North\", \"Wind_Direction\"] = \"N\"\n",
    "df.loc[df[\"Wind_Direction\"] == \"South\", \"Wind_Direction\"] = \"S\"\n",
    "df.loc[df[\"Wind_Direction\"] == \"West\", \"Wind_Direction\"] = \"W\"\n",
    "\n",
    "df[\"Wind_Direction\"] = df[\"Wind_Direction\"].map(lambda x : x if len(x) != 3 else x[1:], na_action=\"ignore\")\n",
    "\n",
    "df[\"Wind_Direction\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a72c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the Missing Values in Data :\n",
    "\n",
    "print(\"Data : \")\n",
    "display(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da7f6138",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_size=len(df)\n",
    "\n",
    "train_size=math.floor(0.66*total_size) \n",
    "display\n",
    "#training dataset\n",
    "train=df.head(train_size)\n",
    "#test dataset\n",
    "test=df.head(len(df) -train_size)\n",
    "display('Total Size:',total_size)\n",
    "display('Train Size:',train_size)\n",
    "\n",
    "display('Train Head :',train)\n",
    "display('Test Head :',test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e40b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train[['Side','Wind_Direction','Day','Month','Year','Hour']]\n",
    "\n",
    "y_train = train['Severity']\n",
    "y_train = y_train.to_frame()\n",
    "\n",
    "X_test = test[['Side','Wind_Direction','Day','Month','Year','Hour']]\n",
    "y_test = test['Severity']\n",
    "y_test = y_test.to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9b1974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_encoding( encoding_strategy , encoding_data , encoding_columns ):\n",
    "    \n",
    "    if encoding_strategy == \"LabelEncoding\":\n",
    "        print(\"IF LabelEncoding\")\n",
    "        Encoder = LabelEncoder()\n",
    "        for column in encoding_columns :\n",
    "            print(\"column\",column )\n",
    "            encoding_data[ column ] = Encoder.fit_transform(tuple(encoding_data[ column ]))\n",
    "        \n",
    "    elif encoding_strategy == \"OneHotEncoding\":\n",
    "        print(\"ELIF OneHotEncoding\")\n",
    "        encoding_data = pd.get_dummies(encoding_data)\n",
    "        \n",
    "    dtypes_list =['float64','float32','int64','int32']\n",
    "    encoding_data.astype( dtypes_list[0] ).dtypes\n",
    "    \n",
    "    return encoding_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba3411d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [\"Red\",\"Blue\",\"Green\",\"Red\",\"Blue\",\"Blue\"] \n",
    "  \n",
    "df = pd.DataFrame(data, columns = ['Color']) \n",
    "  \n",
    "print(\"Before One Hot Encoding : \")\n",
    "display(df)\n",
    "print(\"\\nAfter One Hot Encoding : \")\n",
    "display( pd.get_dummies(df) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95517e16",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding_columns  = [ 'Side','Day', 'Week', 'Month', 'Hour' ]\n",
    "encoding_strategy = [ \"LabelEncoding\", \"OneHotEncoding\"]\n",
    "\n",
    "X_train_encode = data_encoding( encoding_strategy[1] , X_train , encoding_columns )\n",
    "X_test_encode =  data_encoding( encoding_strategy[1] , X_test  , encoding_columns )\n",
    "\n",
    "# Display Encoded Train and Test Features :\n",
    "\n",
    "display(X_train_encode.head())\n",
    "display(X_test_encode.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b9e83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_scaling( scaling_strategy , scaling_data , scaling_columns ):\n",
    "    \n",
    "    if    scaling_strategy ==\"RobustScaler\" :\n",
    "        scaling_data[scaling_columns] = RobustScaler().fit_transform(scaling_data[scaling_columns])\n",
    "        \n",
    "    elif  scaling_strategy ==\"StandardScaler\" :\n",
    "        scaling_data[scaling_columns] = StandardScaler().fit_transform(scaling_data[scaling_columns])\n",
    "        \n",
    "    elif  scaling_strategy ==\"MinMaxScaler\" :\n",
    "        scaling_data[scaling_columns] = MinMaxScaler().fit_transform(scaling_data[scaling_columns])\n",
    "        \n",
    "    elif  scaling_strategy ==\"MaxAbsScaler\" :\n",
    "        scaling_data[scaling_columns] = MaxAbsScaler().fit_transform(scaling_data[scaling_columns])\n",
    "        \n",
    "    else :  # If any other scaling send by mistake still perform Robust Scalar\n",
    "        scaling_data[scaling_columns] = RobustScaler().fit_transform(scaling_data[scaling_columns])\n",
    "    \n",
    "    return scaling_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d1db94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RobustScaler is better in handling Outliers :\n",
    "\n",
    "scaling_strategy = [\"RobustScaler\", \"StandardScaler\",\"MinMaxScaler\",\"MaxAbsScaler\"]\n",
    "X_train = data_scaling( scaling_strategy[0] , X_train_encode , X_train_encode.columns )\n",
    "X_test  = data_scaling( scaling_strategy [0] , X_test_encode  , X_test_encode.columns )\n",
    "\n",
    "# Display Scaled Train and Test Features :\n",
    "\n",
    "display(X_train.head())\n",
    "display(X_test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7134481c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LogisticRegression(max_iter=10000,random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "accuracy_train = clf.score(X_train, y_train)\n",
    "accuracy_test = clf.score(X_test,y_test)\n",
    "print(\"Train Accuracy: %.1f%%\"% (accuracy_train*100))\n",
    "print(\"Test Accuracy: %.1f%%\"% (accuracy_test*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab480cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the f1 score\n",
    "\n",
    "lr_cal = clf.predict(X_test)\n",
    "\n",
    "# Calculate the f1 score\n",
    "f1_lr = f1_score(y_test, lr_cal, average='weighted') \n",
    "print(\"F1 Score: %3.4f\" %(f1_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a61966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step, on X_train with y_train\n",
    "tree_clf = tree.DecisionTreeClassifier(min_samples_split = 5)\n",
    "tree_clf = tree_clf.fit(X_train,y_train)\n",
    "\n",
    "tree_accuracy_train = tree_clf.score(X_train, y_train)\n",
    "tree_accuracy_test = tree_clf.score(X_test,y_test)\n",
    "print(\"Train Accuracy: %.1f%%\"% (tree_accuracy_train*100))\n",
    "print(\"Test Accuracy: %.1f%%\"% (tree_accuracy_test*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d19950",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_cal = tree_clf.predict(X_test)\n",
    "\n",
    "# Calculate the f1 score\n",
    "f1_tree = f1_score(y_test, tree_cal, average='weighted') \n",
    "print(\"F1 Score: %3.4f\" %(f1_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bdc924e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "rf_clf=RandomForestClassifier(n_estimators=10)\n",
    "rf_clf.fit(X_train,y_train)\n",
    "\n",
    "train_pred =  rf_clf.predict(X_train)\n",
    "test_pred =rf_clf.predict(X_test)\n",
    "\n",
    "rf_train_accuracy = accuracy_score(y_train, train_pred)\n",
    "rf_test_accuracy = accuracy_score(y_test, test_pred)\n",
    "\n",
    "print(\"Train Accuracy: %.1f%%\"% (rf_train_accuracy*100))\n",
    "print(\"Test Accuracy: %.1f%%\"% (rf_test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7645c5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_cal = tree_clf.predict(X_test)\n",
    "\n",
    "# Calculate the f1 score\n",
    "f1_rf = f1_score(y_test, rf_cal, average='weighted') \n",
    "print(\"F1 Score: %3.4f\" %(f1_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a716c412",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(n_estimators=100)\n",
    "\n",
    "xgb_clf.fit(X_train,y_train)\n",
    "\n",
    "# predict the target on the train & test  dataset\n",
    "predict_train = xgb_clf.predict(X_train)\n",
    "predict_test = xgb_clf.predict(X_test)\n",
    "\n",
    "# Accuracy Score on train & test dataset\n",
    "\n",
    "xgb_accuracy_train = accuracy_score(y_train,predict_train)\n",
    "xgb_accuracy_test = accuracy_score(y_test,predict_test)\n",
    "\n",
    "print('Train Accuracy: %.1f' %(xgb_accuracy_train*100) )\n",
    "print('Test Accuracy:%.1f' %(xgb_accuracy_test*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f63e475",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "xgb_cal = xgb_clf.predict(X_test)\n",
    "\n",
    "# Calculate the f1 score\n",
    "f1_xgb = f1_score(y_test, xgb_cal, average='weighted') \n",
    "print(\"F1 Score: %3.4f\" %(f1_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6f99f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the scores on training and test set\n",
    "\n",
    "print('Training set score: {:.4f}'.format(clf.score(X_train, y_train)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd60b1be",
   "metadata": {},
   "source": [
    "# XGBOOST REGRESSOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6004969",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "xgb = XGBRegressor()\n",
    "xgb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c858cdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725c9626",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f183215",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "score = cross_val_score(xgb, X_train, y_train, cv = 5, verbose = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea7e970",
   "metadata": {},
   "outputs": [],
   "source": [
    "score.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76c202a9",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7773471",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc235ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab21fce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xgb_residual = y_test - xgb_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bb20aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sns.distplot(xgb_residual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576b8163",
   "metadata": {},
   "source": [
    "# Hypermeter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b65445",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGBRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "442f034e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3687b4c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameter = dict(booster=['gbtree', \"dart\", \"gblinear\"], gamma=[0,1,2,3,5,5,6,7,8,9,10], n_estimators=[int(x) for x in np.linspace(100,1200,num = 12)], min_child_weight=[1,3,5,6,7,8], max_depth=[int(x) for x in np.linspace(5,30, num = 6)], learning_rate=[0.05,0.1, 0.2, 0.5, 0.6,0.8, 0.9, 1],subsample=[0.1,0.3,0.5,0.7,0.8, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f91aeda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor = RandomizedSearchCV(xgb, parameter, n_iter= 100, scoring  = \"neg_mean_squared_error\", verbose = 3, cv = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5905512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b478d846",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c438875e",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regressor.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ff372a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_regress_predict = xgb_regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e9a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "print(\"ACCURACY:\", r2_score(y_test, xgb_regress_predict))\n",
    "print(\"RMSE:\", np.sqrt(mean_squared_error(y_test, xgb_regress_predict)))\n",
    "print(\"MSE:\", mean_squared_error(y_test, xgb_regress_predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce2ca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "residual_xgb = y_test - xgb_regress_predict\n",
    "sns.distplot(residual_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b5b216",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "xgb_regressor_file = open(\"xgb_regressor.pkl\", \"wb\")\n",
    "pickle.dump(xgb_regressor, xgb_regressor_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9601290c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the lightgbm model\n",
    "import lightgbm as lgb\n",
    "clf = lgb.LGBMClassifier(n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "# predict the results\n",
    "y_pred=clf.predict(X_test)\n",
    "# predict the target on the train & test  dataset\n",
    "predict_train = clf.predict(X_train)\n",
    "predict_test = clf.predict(X_test)\n",
    "\n",
    "# Accuracy Score on train & test dataset\n",
    "\n",
    "accuracy_train = accuracy_score(y_train,predict_train)\n",
    "accuracy_test = accuracy_score(y_test,predict_test)\n",
    "\n",
    "print('Train Accuracy: %.1f' %(accuracy_train*100) )\n",
    "print('Test Accuracy:%.1f' %(accuracy_test*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74cae64f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy=accuracy_score(y_pred, y_test)\n",
    "print('LightGBM Model accuracy score: {0:0.4f}'.format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46946ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6445b070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the scores on training and test set\n",
    "\n",
    "print('Training set score: {:.4f}'.format(clf.score(X_train, y_train)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(clf.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f82cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import linear_model\n",
    "from dbn.models import BinaryRBM, UnsupervisedDBN, SupervisedDBNRegression\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "accident = pd.read_csv(\"C:/Users/Admin/Pictures/11103/LSTM with GWO/dataset/US_Accidents_Dec20_updated.csv\")\n",
    "\n",
    "accident.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ef727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing \n",
    "label_encoder = preprocessing.LabelEncoder()\n",
    "accident['ID']= label_encoder.fit_transform(accident['ID'])\n",
    "accident['Street']= label_encoder.fit_transform(accident['Street'])\n",
    "accident['Side']= label_encoder.fit_transform(accident['Side'])\n",
    "accident['City']= label_encoder.fit_transform(accident['City'])\n",
    "\n",
    "accident['County']= label_encoder.fit_transform(accident['County'])\n",
    "accident['State']= label_encoder.fit_transform(accident['State'])\n",
    "accident['Country']= label_encoder.fit_transform(accident['Country'])\n",
    "accident['Timezone']= label_encoder.fit_transform(accident['Timezone'])\n",
    "accident['Airport_Code']= label_encoder.fit_transform(accident['Airport_Code'])\n",
    "accident['Wind_Direction']= label_encoder.fit_transform(accident['Wind_Direction'])\n",
    "accident['Weather_Condition']= label_encoder.fit_transform(accident['Weather_Condition'])\n",
    "accident['Amenity']= label_encoder.fit_transform(accident['Amenity'])\n",
    "accident['Bump']= label_encoder.fit_transform(accident['Bump'])\n",
    "accident['Crossing']= label_encoder.fit_transform(accident['Crossing'])\n",
    "accident['Give_Way']= label_encoder.fit_transform(accident['Give_Way'])\n",
    "accident['Junction']= label_encoder.fit_transform(accident['Crossing'])\n",
    "accident['Give_Way']= label_encoder.fit_transform(accident['Junction'])\n",
    "accident['No_Exit']= label_encoder.fit_transform(accident['No_Exit'])\n",
    "accident['Railway']= label_encoder.fit_transform(accident['Railway'])\n",
    "#final_data['Railway']= label_encoder.fit_transform(final_data['Railway'])\n",
    "accident['Roundabout']= label_encoder.fit_transform(accident['Roundabout'])\n",
    "accident['Station']= label_encoder.fit_transform(accident['Station'])\n",
    "accident['Stop']= label_encoder.fit_transform(accident['Stop'])\n",
    "accident['Traffic_Calming']= label_encoder.fit_transform(accident['Traffic_Calming'])\n",
    "accident['Traffic_Signal']= label_encoder.fit_transform(accident['Traffic_Signal'])\n",
    "accident['Turning_Loop']= label_encoder.fit_transform(accident['Turning_Loop'])\n",
    "accident['Sunrise_Sunset']= label_encoder.fit_transform(accident['Sunrise_Sunset'])\n",
    "accident['Civil_Twilight']= label_encoder.fit_transform(accident['Civil_Twilight'])\n",
    "accident['Nautical_Twilight']= label_encoder.fit_transform(accident['Nautical_Twilight'])\n",
    "accident['Astronomical_Twilight']= label_encoder.fit_transform(accident['Astronomical_Twilight'])\n",
    "X = accident.drop(['Description','ID','Start_Time','End_Time','Weather_Timestamp','Airport_Code','Number','Zipcode'],axis = 1)\n",
    "from sklearn import linear_model, datasets, metrics\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1764eb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "kmeans = KMeans(2)\n",
    "kmeans.fit(X)\n",
    "print(kmeans.fit)\n",
    "Y = kmeans.fit_predict(X)\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b353f67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Data scaling\n",
    "X = np.asarray(X, 'float32')\n",
    "print(X)\n",
    "print(X.shape)\n",
    "# Splitting data\n",
    "X_train=np.asarray(X, 'float32')\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "print(scaler)\n",
    "X_trainScale = scaler.transform(X_train)\n",
    "print(X_trainScale)\n",
    "X = (X - np.min(X, 0)) / (np.max(X, 0) + 0.0001)  # 0-1 scaling\n",
    "\n",
    "# Splitting data\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "# Training\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a86365",
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic = linear_model.LogisticRegression()\n",
    "dbn = UnsupervisedDBN(hidden_layers_structure=[256, 256],\n",
    "                      batch_size=32,\n",
    "                      learning_rate_rbm=0.005,\n",
    "                      n_epochs_rbm=5,\n",
    "                      activation_function='relu')\n",
    "\n",
    "#classifier =  make_pipeline(steps=[('dbn', dbn),('logistic', logistic)])\n",
    "#classifier =  make_pipeline(steps=[('dbn', dbn),('logistic', logistic)])\n",
    "pipeline = Pipeline([('dbn', dbn),('logistic', logistic)])\n",
    "pipeline.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb50f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "Y_pred = pipeline.predict(X_test)\n",
    "print('Done.\\nAccuracy: %f' % accuracy_score(Y_test, Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525fdde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view confusion-matrix\n",
    "# Print the Confusion Matrix and slice it into four pieces\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print('Confusion matrix\\n\\n', cm)\n",
    "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
    "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
    "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
    "print('\\nFalse Negatives(FN) = ', cm[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e036fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2024f355",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77c8014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6245f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919aef16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
